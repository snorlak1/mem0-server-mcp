# =============================================================================
# Mem0 MCP Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# LLM Provider Configuration
# -----------------------------------------------------------------------------
# Choose your LLM provider: ollama, openai, or anthropic
LLM_PROVIDER=ollama

# -----------------------------------------------------------------------------
# Ollama Configuration (Self-Hosted, Free)
# -----------------------------------------------------------------------------
# Update this to your Ollama server address
OLLAMA_BASE_URL=http://192.168.1.2:11434

# LLM model for memory extraction
OLLAMA_LLM_MODEL=qwen3:8b

# Embedding model for semantic search
OLLAMA_EMBEDDING_MODEL=qwen3-embedding:8b

# Embedding dimensions (CRITICAL: Must match your model)
# qwen3-embedding:8b = 4096 dims
# nomic-embed-text = 768 dims
# all-minilm = 384 dims
OLLAMA_EMBEDDING_DIMS=4096

# -----------------------------------------------------------------------------
# OpenAI Configuration (Cloud, Paid)
# -----------------------------------------------------------------------------
# Uncomment and set if using OpenAI
# OPENAI_API_KEY=sk-proj-...
# OPENAI_LLM_MODEL=gpt-4o
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small
# OPENAI_EMBEDDING_DIMS=1536

# -----------------------------------------------------------------------------
# Anthropic Configuration (Cloud, Paid)
# -----------------------------------------------------------------------------
# Uncomment and set if using Anthropic for LLM
# Note: Anthropic doesn't provide embeddings, so you'll need OpenAI or Ollama for that
# ANTHROPIC_API_KEY=sk-ant-...
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------
# PostgreSQL settings (default values work out of the box)
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=postgres
POSTGRES_PORT=5432
POSTGRES_COLLECTION_NAME=memories

# Neo4j settings (default values work out of the box)
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=mem0graph
NEO4J_HTTP_PORT=7474
NEO4J_BOLT_PORT=7687

# -----------------------------------------------------------------------------
# MCP Server Configuration
# -----------------------------------------------------------------------------
# Port for the MCP server (Claude Code connects here)
MCP_PORT=8080

# Port for the Mem0 REST API
MEM0_PORT=8000

# Default user ID (used when PROJECT_ID_MODE=global or manual)
DEFAULT_USER_ID=claude_code_mcp

# Project Isolation Mode:
# - "auto"   : Auto-detect project from working directory (RECOMMENDED)
# - "manual" : Use DEFAULT_USER_ID explicitly per project
# - "global" : Share memories across all projects
PROJECT_ID_MODE=auto

# Project directory (for auto mode, leave as default)
PROJECT_DIR=/app

# -----------------------------------------------------------------------------
# Optional: Logging and Other Settings
# -----------------------------------------------------------------------------
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# -----------------------------------------------------------------------------
# Notes
# -----------------------------------------------------------------------------
# 1. For Ollama: Ensure models are pulled on your Ollama server:
#    ollama pull qwen3:8b
#    ollama pull qwen3-embedding:8b
#
# 2. Embedding dimensions MUST match your model:
#    - Qwen3-embedding: 4096 dims (HNSW disabled, slower search)
#    - OpenAI text-embedding-3-small: 1536 dims (HNSW enabled, fast)
#    - Nomic-embed-text: 768 dims (HNSW enabled, fast)
#
# 3. HNSW is automatically disabled for dimensions > 2000
#
# 4. For faster performance with large datasets, use smaller embedding models
#
# 5. Project isolation (auto mode) creates unique user IDs per project directory
