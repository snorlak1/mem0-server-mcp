# =============================================================================
# Mem0 MCP Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# LLM Provider Configuration
# -----------------------------------------------------------------------------
# Choose your LLM provider: ollama, openai, or anthropic
LLM_PROVIDER=ollama

# -----------------------------------------------------------------------------
# Ollama Configuration (Self-Hosted, Free)
# -----------------------------------------------------------------------------
# Update this to your Ollama server address
OLLAMA_BASE_URL=http://192.168.1.2:11434

# LLM model for memory extraction
OLLAMA_LLM_MODEL=qwen3:8b

# Embedding model for semantic search
# RECOMMENDED: qwen3-embedding:0.6b (1024 dims, HNSW enabled, fast)
OLLAMA_EMBEDDING_MODEL=qwen3-embedding:0.6b

# Embedding dimensions (CRITICAL: Must match your model)
# qwen3-embedding:0.6b = 1024 dims (RECOMMENDED - HNSW enabled, fast)
# qwen3-embedding:4b = 2560 dims (HNSW disabled, slower)
# qwen3-embedding:8b = 4096 dims (HNSW disabled, slowest)
# nomic-embed-text = 768 dims (HNSW enabled, fast)
# all-minilm = 384 dims (HNSW enabled, fast)
OLLAMA_EMBEDDING_DIMS=1024

# -----------------------------------------------------------------------------
# OpenAI Configuration (Cloud, Paid)
# -----------------------------------------------------------------------------
# Uncomment and set if using OpenAI
# OPENAI_API_KEY=sk-proj-...
# OPENAI_LLM_MODEL=gpt-4o
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small
# OPENAI_EMBEDDING_DIMS=1536

# -----------------------------------------------------------------------------
# Anthropic Configuration (Cloud, Paid)
# -----------------------------------------------------------------------------
# Uncomment and set if using Anthropic for LLM
# Note: Anthropic doesn't provide embeddings, so you'll need OpenAI or Ollama for that
# ANTHROPIC_API_KEY=sk-ant-...
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------
# PostgreSQL settings (default values work out of the box)
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=postgres
POSTGRES_PORT=5432
POSTGRES_COLLECTION_NAME=memories

# Neo4j settings (default values work out of the box)
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=mem0graph
NEO4J_HTTP_PORT=7474
NEO4J_BOLT_PORT=7687

# -----------------------------------------------------------------------------
# MCP Server Configuration
# -----------------------------------------------------------------------------
# Port for the MCP server (Claude Code connects here)
MCP_PORT=8080

# Port for the Mem0 REST API
MEM0_PORT=8000

# Default user ID (used when PROJECT_ID_MODE=global or manual)
DEFAULT_USER_ID=claude_code_mcp

# Project Isolation Mode:
# - "auto"   : Auto-detect project from working directory (RECOMMENDED)
# - "manual" : Use DEFAULT_USER_ID explicitly per project
# - "global" : Share memories across all projects
PROJECT_ID_MODE=auto

# Project directory (for auto mode, leave as default)
PROJECT_DIR=/app

# -----------------------------------------------------------------------------
# Smart Text Chunking Configuration
# -----------------------------------------------------------------------------
# Maximum characters per chunk (default: 1000)
# Larger values = fewer chunks, faster processing, higher timeout risk
# Smaller values = more chunks, slower processing, safer for large text
CHUNK_MAX_SIZE=1000

# Overlap between chunks in characters (default: 150)
# Larger values = better context preservation, more duplicate storage
# Smaller values = less context, less duplication
CHUNK_OVERLAP_SIZE=150

# -----------------------------------------------------------------------------
# Optional: Logging and Other Settings
# -----------------------------------------------------------------------------
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# -----------------------------------------------------------------------------
# Notes
# -----------------------------------------------------------------------------
# 1. For Ollama: Ensure models are pulled on your Ollama server:
#    ollama pull qwen3:8b
#    ollama pull qwen3-embedding:0.6b  # RECOMMENDED
#
# 2. Embedding dimensions MUST match your model:
#    - qwen3-embedding:0.6b: 1024 dims (HNSW enabled, fast) ✅ RECOMMENDED
#    - qwen3-embedding:4b: 2560 dims (HNSW disabled, slower)
#    - qwen3-embedding:8b: 4096 dims (HNSW disabled, slowest)
#    - OpenAI text-embedding-3-small: 1536 dims (HNSW enabled, fast)
#    - Nomic-embed-text: 768 dims (HNSW enabled, fast)
#
# 3. HNSW indexing is automatically:
#    - ENABLED for dimensions ≤ 2000 (4x faster search)
#    - DISABLED for dimensions > 2000 (slower search)
#
# 4. For best performance, use models with ≤ 2000 dimensions
#
# 5. Project isolation (auto mode) creates unique user IDs per project directory
#
# 6. Smart chunking automatically handles large text inputs:
#    - Splits at semantic boundaries (paragraphs → sentences)
#    - Preserves context with overlap between chunks
#    - Configurable via CHUNK_MAX_SIZE and CHUNK_OVERLAP_SIZE
